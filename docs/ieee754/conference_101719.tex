\documentclass[conference]{IEEEtran}

\makeatletter

\def\ps@IEEEtitlepagestyle{%
  \def\@oddfoot{\mycopyrightnotice}%
  \def\@evenfoot{}%
}
\def\mycopyrightnotice{%
  {\footnotesize XXX-X-XXXX-XXXX-X/XX/\$XX.00~\copyright~20XX IEEE\hfill}% <--- Change here
  \gdef\mycopyrightnotice{}
}


\usepackage{blindtext}
\usepackage{eso-pic}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{eso-pic}
\newcommand\AtPageUpperMyright[1]{\AtPageUpperLeft{%
 \put(\LenToUnit{0.17\paperwidth},\LenToUnit{-2cm}){%
     \parbox{0.9\textwidth}{\raggedleft\fontsize{8}{11}\selectfont #1}}%
 }}%
\newcommand{\conf}[1]{%
\AddToShipoutPictureBG*{%
\AtPageUpperMyright{#1}
}
}    
    
    
\begin{document}
\title{\vspace*{1cm} FaKe: Filipino Language Fake News Detection
System Using Machine Learning\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Rene Andre Jocsing}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences} \\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
rbjocsing@up.edu.ph}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Chancy Ponce de Leon}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences}\\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
cmponcedeleon@up.edu.ph}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Coebe Austin Lupac}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences} \\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
cvlupac1@up.edu.ph}
\and
\IEEEauthorblockN{4\textsuperscript{th} Francis Dimzon}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences}\\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
fddimzon1@up.edu.ph}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}




\maketitle
\conf{\textit{  Proc. of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA 2026) \\ 
5-7 February 2026, Boracay-Philippines}}
\begin{abstract}
    Methods for curbing the spread of misinformation in the Philippines remain inadequate. The internet as a medium for fake news necessitates fast, automated, and accessible countermeasures. A precursor study from 2020 benchmarks Transfer Learning (TL) techniques in building Filipino language fake news classifiers from a low-resource dataset. Despite promising results, the models from the aforementioned study cannot be easily deployed for a wide audience. In this work, we show that robust fake news classifiers for a morphologically rich language can be constructed from lightweight machine learning models and a low-resource dataset. We show that these machine learning models can be successfully deployed on a stringent infrastructure. First, we construct a dataset of Filipino language news articles. We extract Filipino linguistic features from the dataset. Next, we train several machine learning models and find that the Logistic Regression model achieves the top accuracy of 95\%. Using this model, we build and deploy a system for classifying Filipino language fake news.
\end{abstract}

%\copyrightnotice{XXX-X-XXXX-XXXX-X/XX/\$XX.00 ©20XX IEEE}




\begin{IEEEkeywords}
natural language processing, machine learning, fake news in Filipino language, Filipino linguistic features
\end{IEEEkeywords}

\section{Introduction}
Misinformation ranks among the world's top global risks as fake news outlets see traffic. In the Philippines, journalists and political analysts speculate that Former President Rodrigo Duterte's landslide victory in the 2016 presidential elections has been brought about by paid trolls disseminating fake news through social media outlets \cite{b1}. Former President Duterte earned publicity by popularizing a depiction of the Philippines as a \textit{narco-state}. Though he insists that this rhetoric is truth, the United Nations Office on Drugs and Crime reports otherwise—the Philippines' drug use prevalence is lower than the global average \cite{b2}.

A precursor study \cite{b3} explores Transfer Learning (TL) in building robust fake news classifiers for the morphologically rich Filipino language. Despite promising results, the deployment of the models (e.g., BERT) used in this work costs somewhere around 50,000 to 1,600,000 USD \cite{b4}. Hence, they cannot be easily deployed for consumer-level applications \cite{b3, b4}.

This work investigates lightweight machine learning models in tackling the classification of Filipino language fake news. A low-resource dataset of Filipino language news articles (Fake News Filipino 2024) is sourced from the internet to alleviate resource scarcity. A Logistic Regression (LR) model is trained on the joint Fake News Filipino and Fake News Filipino 2024 dataset, achieving a top accuracy of 95\%. The authors successfully deploy the fake news classifier as a browser extension (dubbed as FaKe) on a stringent infrastructure.

\section{Related Work}

Machine learning models have shown promise in automating fake news detection. A study \cite{b5} reports that Naive Bayes with \textit{n}-grams achieves 93\% accuracy on English datasets, while another study \cite{b6} finds that Support Vector Classifier (SVC) reaches 96\% accuracy on a large corpora of about 20,000 articles. However, these studies focus on resource-rich English datasets.

For Filipino language news articles, a study \cite{b3} pioneers fake news detection using Byte-Pair Encoding (BPE) tokenization to handle morphologically rich language features and out of vocabulary (OOV) words. While another study \cite{b7} identifies 76 linguistic features for fake news detection in the Filipino context, its corpus uses English-language articles. Two studies \cite{b8, b9} develop Filipino-specific readability metrics and linguistic features including traditional features, syllabic patterns based on Philippine orthography, and morphological features from verb inflections. These studies identify polysyllable word count, sentence count, and average sentence length as top predictors for Filipino text readability. As of the time of writing, no standardized instrument or assessment tool exists for measuring the readability of texts in the Filipino language \cite{b8}. However, a study \cite{b10} proposes a formula for assessing the readability of modern Filipino texts, wherein readability score is a function of word count and words per sentence.

Despite these advances, Filipino linguistic features have not been applied in training Filipino language fake news classifiers. Moreover, deployment costs for complex models remain prohibitive (\$50,000 to \$1,600,000) \cite{b4}, necessitating the investigation of lightweight models for Filipino language fake news detection.

\section{Methodology}

\subsection{Data Collection}
Following a precursor study \cite{b3}, the authors constructed a new dataset of Filipino language news articles, Fake News Filipino 2024, containing 3,206 articles (1,603 fake, 1,603 real). Fake articles were sourced from fraudulent sites tagged by  VERA Files \cite{b11, b12} while real articles were sourced from mainstream sources like ABS-CBN \cite{b13} and Philippine Star \cite{b14}. The news articles were encoded in UTF-8 format with minimal preprocessing to preserve features such as misspellings and punctuation.

\subsection{VERA Files Fact-Check}

VERA Files is a Philippine nonprofit media organization that conducts fact-checking of statements of public officials and online information. The organization selects claims eligible for fact-checking based on three criteria: (1) the statement must be factual rather than opinion, (2) verification must be feasible using available records and expert sources, and (3) the claim must be of public interest. For viral online content, reach is also considered \cite{b15}.

VERA Files employs a rigorous six-step verification process involving multiple team members, cross-checking sources, and editorial review before publication. These steps are the following: (1) monitoring public statements, (2) assigning research to verify claims using multiple sources, (3) collaborative review by the entire team, (4) additional fact-checking and editing, (5) preparation of supporting materials, and (6) final sign-off by senior editors before publication \cite{b15}. The organization adheres to the International Fact-Checking Network's code of principles \cite{b16} and maintains strict transparency by providing links to primary sources in all fact-check articles.

\subsection{Feature Extraction and Model Training}
Text was tokenized using BPE, adhering to the methodology of the precursor study \cite{b3}. The authors extracted the following Filipino linguistic features \cite{b8,b9,b10}: traditional features (TRAD), syllabic features (SYLL), lexical features (LEX), and morphological features (MORPH). OOV words and stop words (SW) were also extracted. Macahilig's readability formula \cite{b10} was also used to extract readability scores from the corpus (READ). Tokens were vectorized with Term Frequency-Inverse Document Frequency (TF-IDF) and bag of words (BOW). Table \ref{tab:features} shows a summary of the extracted linguistic features.

\begin{table*}[!t]
\centering
\caption{Descriptions and predictors of features.}
\label{tab:features}
\footnotesize
\begin{tabular}{|p{0.18\textwidth}|p{0.38\textwidth}|p{0.38\textwidth}|}
\hline
\textbf{Feature} & \textbf{Description} & \textbf{Predictors} \\ \hline

Bag-of-words & Unordered collection of words. & Bag-of-words model of the input text. \\ \hline

TF-IDF & Word to document ratio in the corpus. & TF-IDF using \textit{n}-gram values of \{1, 2, 3\}. \\ \hline

Misspelled words & Misspelled words in the input text. & Total misspelled word count. \\ \hline

OOV words & Words in the input text that are not found in the Filipino and English dictionary used in training. & Total OOV word count. \\ \hline

Stop words & Words considered insignificant in processing text input e.g. ``at'', ``ay'', ``din'', etc. in Filipino or ``a'', ``the'', ``to'', ``as'' in English. & Total stop word count. \\ \hline

Readability Score & Score determined by the readability formula for Filipino text by \cite{b10}. The higher the readability index, the more difficult the text is to read. & Readability is computed by: \(P = -4.161 + 0.280(w/s) + 0.106(cw)\) where \(P\) is the readability index, \(w/s\) is the average number of words per sentence, and \(cw\) is the word count. \\ \hline

Traditional & Traditional or surface-based features based on counts and frequencies. & Total word, sentence, phrase, polysyllabic word counts. Average word length, sentence length, syllable per word. \\ \hline

Syllabic Pattern & Syllable pattern densities based on the prescribed Philippine orthography. & Prescribed Philippine orthography \{v, cv, vc, cvc, vcc, cvcc, ccvc, ccvcc, ccvccc\}, where c and v are consonant and vowel notations. \\ \hline

Lexical & Lexical or context-carrying features using part-of-speech categories. & Type-token variations (regular, logarithmic, corrected, root). Noun and verb token ratio. Lexical density. Foreign word and compound word density. \\ \hline

Morphological & Morphological features based on verb inflection. & Densities of foci of verbs based on tense: \{actor, object, benefactive, locative, instrumental, referential\}. Densities of various foci of verbs based on aspect: \{infinitive, perfective, imperfective, contemplative, participle, recent-past, auxiliary\}. \\ \hline

\end{tabular}
\end{table*}

The authors trained four classifiers: Multinomial Naive Bayes (MNB), Logistic Regression, Random Forest (RF), and Support Vector Classifier. Grid search with five-fold cross-validation determined optimal hyperparameters. The classifiers were evaluated 30 times (six runs of five-fold cross validation) across three datasets: Fake News Filipino \cite{b3}, Fake News Filipino 2024, and their joint corpus. Two-way ANOVA with Bonferroni correction assessed performance differences and found these differences between models to be significant.

\subsection{System Deployment}
The best model was deployed as a web service using FastAPI on Render's \cite{b17} \textbf{free} tier, interfaced through Tampermonkey \cite{b18}. Tampermonkey allowed users to enhance the functionality of web pages by injecting a script (written in JavaScript) into the page. It was chosen as it was one of the most popular browser extension managers available on mainstream web browsers (e.g., Google Chrome, Mozilla Firefox, Microsoft Edge, Safari, Opera, Next). Tampermonkey granted web developers the ability to write browser-independent code and provided a way to easily install cross-browser web extensions.

The browser extension, FaKe, scrapes article text, sends it to the service for feature extraction and classification, then displays results. Due to infrastructure constraints, the deployed model excludes LEX and MORPH features (requires deployment of Java runtime and 3+ minutes of processing time per article). The FaKe browser extension, using the best machine learning model, was validated on the Chrome, Edge, and Firefox web browsers.

\section{Results}

\section{Results and Discussion}

\subsection{Dataset Characteristics}
Fake News Filipino 2024 contains 3,206 balanced articles. Compared to Fake News Filipino (2020), it has higher average OOV count (22.2 vs 17.6), lower readability (19.7 vs 20.5), and fewer stop words (74.5 vs 77.8), indicating some stylistic differences between datasets.

\subsection{Model Performance}
Four classifiers were trained with and without optimal hyperparameters, identified via grid search. Table \ref{tab:no_hyperparam_summary} shows performance on the joint corpus without hyperparameter tuning, while Table \ref{tab:hyperparam_summary} shows performance on the joint corpus with hyperparameter tuning. LR and SVC both achieved highest accuracy (93\%), though SVC required hyperparameter tuning while LR did not. MNB improved significantly with tuning (58\% to 87\%). The best-performing model is LR, as it did not require hyperparameter tuning.

\begin{table}[!t]
\caption{Performance metrics for classifiers without hyperparameter tuning.}
\label{tab:no_hyperparam_summary}
\begin{center}
\begin{tabular}{|l|ccc|ccc|}
\hline
& \multicolumn{3}{c|}{\textbf{Multinomial Naive Bayes}} & \multicolumn{3}{c|}{\textbf{Logistic Regression}} \\
\hline
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
\hline
Fake & 1.00 & 0.16 & 0.28 & 0.92 & 0.93 & 0.93 \\
Real & 0.54 & 1.00 & 0.70 & 0.93 & 0.92 & 0.93 \\
Accuracy & & & 0.58 & & & 0.93 \\
\hline
& \multicolumn{3}{c|}{\textbf{Random Forest}} & \multicolumn{3}{c|}{\textbf{Support Vector Classifier}} \\
\hline
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
\hline
Fake & 0.91 & 0.90 & 0.90 & 0.87 & 0.74 & 0.80 \\
Real & 0.90 & 0.91 & 0.90 & 0.77 & 0.89 & 0.83 \\
Accuracy & & & 0.90 & & & 0.81 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[!t]
\caption{Classifier performance with hyperparameter tuning.}
\label{tab:hyperparam_summary}
\begin{center}
\begin{tabular}{|l|ccc|ccc|}
\hline
& \multicolumn{3}{c|}{\textbf{Multinomial NB}} & \multicolumn{3}{c|}{\textbf{Logistic Reg.}} \\
\hline
& \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\hline
Fake & 0.93 & 0.76 & 0.85 & 0.92 & 0.93 & 0.93 \\
Real & 0.80 & 0.97 & 0.88 & 0.93 & 0.92 & 0.93 \\
Accuracy & & & 0.87 & & & 0.93 \\
\hline
& \multicolumn{3}{c|}{\textbf{Random Forest}} & \multicolumn{3}{c|}{\textbf{SVC}} \\
\hline
& \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\hline
Fake & 0.90 & 0.88 & 0.89 & 0.92 & 0.93 & 0.93 \\
Real & 0.88 & 0.90 & 0.89 & 0.93 & 0.92 & 0.93 \\
Accuracy & & & 0.89 & & & 0.93 \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Cross-Dataset Evaluation}
Models were evaluated 30 times across three datasets. Table \ref{tab:average_accuracies} shows average accuracies. LR and SVC achieved 95\% accuracy on individual datasets but dropped to 92\% on the joint corpus, while MNB (92\% to 86\%) and RF (92\% to 88\%) dropped further. The best model, Logistic Regression, achieved a top accuracy of 95\%.

\begin{table}[!t]
\centering
\caption{Average accuracies across datasets.}
\label{tab:average_accuracies}
\begin{tabular}{llr}
\hline
\textbf{Classifier} & \textbf{Dataset} & \textbf{Accuracy} \\
\hline
LR & FN Filipino 2020 & \textbf{0.951} \\
 & FN Filipino 2024 & \textbf{0.947} \\
 & Combined & \textbf{0.924} \\
\hline
MNB & FN Filipino 2020 & 0.923 \\
 & FN Filipino 2024 & 0.885 \\
 & Combined & 0.860 \\
\hline
RF & FN Filipino 2020 & 0.919 \\
 & FN Filipino 2024 & 0.926 \\
 & Combined & 0.888 \\
\hline
SVC & FN Filipino 2020 & \textbf{0.951} \\
 & FN Filipino 2024 & \textbf{0.947} \\
 & Combined & 0.922 \\
\hline
\end{tabular}
\end{table}

Two-way ANOVA revealed significant main effects for dataset ($p<0.001$) and classifier ($p<0.001$), plus interaction ($p<0.001$). Bonferroni post-hoc tests showed LR and SVC performed similarly across all datasets ($p>0.05$), while all classifiers performed significantly worse on the combined corpus ($p<0.001$). This degradation likely stems from inconsistent writing styles between datasets, an inference supported by their differing OOV counts and readability scores.

\subsection{Deployment and Model Interpretation}
The FaKe browser extension (LR without LEX and MORPH features) was validated on Chrome, Edge, and Firefox with uniform results.  The exclusion of LEX and MORPH during cross validation did not significantly affect the accuracy of the deployed model. Analysis of model coefficients revealed readability score (0.37) as strongest positive predictor for real news, while average phrase count (-0.42) most strongly indicated fake news. Specific words also influenced predictions: "source" (2.28) and "gma" (1.32) suggested authenticity, while "upang" (-0.88) and "ngunit" (-0.88) indicated fake news, reflecting patterns learned from the joint corpus. Tables \ref{tab::lang_coef} and \ref{tab::vec_coef} show the coefficients of the predictors.

\begin{table}[!t]
\caption{Linguistic features coefficients.}
\label{tab::lang_coef}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Predictor} & \textbf{Coefficient} \\ 
\hline
ave-phrase-count & $-0.422$ \\
ave-word-length & $-0.346$ \\
cvc-density & $-0.337$ \\ 
consonant-cluster & $-0.319$ \\ 
cvcc-density & $-0.248$ \\ 
vcc-density & $-0.213$ \\ 
vc-density & $-0.134$ \\ 
word-count-per-sentence & $-0.129$ \\ 
v-density & $-0.060$ \\ 
ave-syllable-count-of-word & $-0.060$ \\ 
polysyll-count & $-0.054$ \\ 
ccvcc-density & $-0.054$ \\ 
word-count & $-0.016$ \\ 
cv-density & $-0.014$ \\ 
ccvccc-density & $-0.0001$ \\ 
count-oov-words & $0.025$ \\ 
count-stopwords & $0.025$ \\ 
sentence-count & $0.064$ \\ 
readability-score & $0.367$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[!t]
\caption{Top vectorizer predictors for each class.}
\label{tab::vec_coef}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Predictor} & \textbf{Coefficient} \\ 
\hline
vectorizers--bow--upang & $-0.876$ \\
vectorizers--bow--ngunit & $-0.876$ \\
vectorizers--bow--sinabi & $-0.824$ \\
vectorizers--bow--gma & $1.320$ \\
vectorizers--bow--below & $1.444$ \\
vectorizers--bow--source & $2.279$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Other Studies}

Two previous studies investigated different models for Filipino language fake news classification. The first, precursor study \cite{b3} explored transfer-learning techniques and large models like BERT, achieving an accuracy of 96\% on their best model, while a second, concurrent study \cite{b19} used an improved DistilBERT model to achieve a best accuracy of 94.7\%. Both studies \cite{b3, b19} incorporated the Fake News Filipino \cite{b3} dataset in model training. However, the latter also applied data augmentation through random deletion to generate synthetic data for the dataset \cite{b19}.

While this study's LR model achieved a comparable accuracy (95\% on the Fake News Filipino dataset), it offers significant advantages in deployment feasibility. The DistilBERT model requires 254.18 MB of memory and 66.6M parameters \cite{b19}. In contrast, the LR model has 1.2M parameters and requires much smaller memory: 55.64 MB. It can run on Render's free tier without any GPU requirements. This validates the approach of using lightweight models for resource-constrained, consumer-facing applications.

\section{Conclusion}

In this study, the authors constructed a balanced dataset of fake and real Filipino news articles named Fake News Filipino 2024, containing 1603 instances of real news and 1603 instances of fake news. The authors augmented Fake News Filipino (2020) and compiled a joint corpus for training several machine learning models. They compared the performance of four classifiers: Logistic Regression, Multinomial Naive Bayes, Random Forest, and Support Vector Classifier in classifying news articles across three datasets: Fake News Filipino 2024, Fake News Filipino, and a joint corpus. Hyperparameter tuning was conducted to determine the optimal hyperparameters of each classifier. Logistic Regression emerged as the most suitable model for deployment as it held the highest accuracies, without requiring hyperparameter tuning.

The FaKe browser extension yielded satisfactory results across different browsers. Though it is available for installation on this paper's code repository, there is also a web version that does not require installation on the user's browser. On websites that hinder automated scraping, the web version is more reliable, but it requires manual copy-pasting of articles.

\section{Recommendations}

For future studies, the authors recommend the expansion of the corpus, to include more domains and incorporate more writing styles. The authors recommend the development of a Filipino fake news detection system on a less stringent infrastructure. Furthermore, the authors recommend the undertaking of future studies that tackle other forms of fake news (e.g., spam, tweets, posts, etc.). With respect to deploying a microservice on Render, it is possible to further improve performance by incurring a small monthly fee. Hence, it may be possible to deploy models that use LEX and MORPH features. Articles flagged by the browser extension should have their sources collated into a database. This way, future studies could someday build a dataset that incorporates article headings and article sources as features. Lastly, as the study of Filipino linguistic features is in a stage of infancy, more research should be conducted in the domain.  

\section*{Acknowledgments}

We acknowledge the assistance rendered by Mr. Ron Gerlan F. Naragdao, in providing computational units for training the models in this study. Furthermore, his aid in designing the website for the FaKe browser extension and beta-testing the extension itself was indispensable.

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} J. Lanuza, J. Ong, and R. Tapsell, ``Evolutions of `fake news' from the south: tracking disinformation innovations and interventions between the 2016 and 2019 Philippines elections,'' Berkman Klein Center for Internet \& Society, Harvard University, 2019. [Online]. Available: https://cyber.harvard.edu/sites/default/files/2019-11/Comparative\%20Approaches\%20to\%20Disinformation\%20-\%20Jose\%20Mari\%20Hall\%20Lanuza\%20Slides.pdf
\bibitem{b2} A. Yee, ``Post-truth politics and fake news in Asia,'' Global Asia, vol. 12, pp. 66--71, 2017.
\bibitem{b3} J. C. B. Cruz, J. A. Tan, and C. Cheng, ``Localization of fake news detection via multitask transfer learning,'' in Proc. 12th Language Resources and Evaluation Conf., 2020, pp. 2596--2604.
\bibitem{b4} A. Paleyes, R.-G. Urma, and N. D. Lawrence, ``Challenges in deploying machine learning: a survey of case studies,'' ACM Comput. Surv., vol. 55, no. 6, pp. 1--29, 2022.
\bibitem{b5} J. Y. Khan, Md. T. Khondaker, S. Afroz, G. Uddin, and A. Iqbal, ``A benchmark study of machine learning models for online fake news detection,'' Mach. Learn. Appl., vol. 4, p. 100032, 2021.
\bibitem{b6} D. Choudhury and T. Acharjee, ``A novel approach to fake news detection in social networks using genetic algorithm applying machine learning classifiers,'' Multimedia Tools Appl., vol. 82, no. 6, pp. 9029--9045, 2022.
\bibitem{b7} A. C. T. Fernandez and M. Devaraj, ``Computing the linguistic-based cues of fake news in the Philippines towards its detection,'' in Proc. 9th Int. Conf. Web Intelligence, Mining and Semantics, 2019, pp. 1--9.
\bibitem{b8} J. M. Imperial and E. Ong, ``Exploring hybrid linguistic feature sets to measure Filipino text readability,'' in Proc. Int. Conf. Asian Language Processing (IALP), 2020, pp. 175--180.
\bibitem{b9} J. M. Imperial and E. Ong, ``Diverse linguistic features for assessing reading difficulty of educational Filipino texts,'' arXiv:2108.00241, 2021.
\bibitem{b10} H. B. Macahilig, ``A content-based readability formula for Filipino texts,'' The Normal Lights, vol. 8, 2015.
\bibitem{b11} ``VERA Files: about.'' [Online]. Available: https://verafiles.org/about. Accessed: May 10, 2024.
\bibitem{b12} ``VERA Files: fact-check.'' [Online]. Available: https://verafiles.org/fact-check. Accessed: May 10, 2024.
\bibitem{b13} ``ABS-CBN News.'' [Online]. Available: https://www.abs-cbn.com/news
\bibitem{b14} ``The Philippine Star.'' [Online]. Available: https://www.philstar.com/
\bibitem{b15} ``What you want to know about `VERA FILES FACT CHECK','' VERA Files. [Online]. Available: https://verafiles.org/articles/what-you-want-know-about-vera-files-fact-check.
\bibitem{b16} ``International Fact-Checking Network code of principles,'' Poynter Institute. [Online]. Available: https://www.ifcncodeofprinciples.poynter.org/how-does-vera-files-rate-its-fact-checks
\bibitem{b17} ``Render: cloud application platform.'' [Online]. Available: https://render.com/
\bibitem{b18} ``Tampermonkey.'' [Online]. Available: https://www.tampermonkey.net/
\bibitem{b19} A. C. B. Sabado, R. J. E. Policarpio, J. M. R. C. Espiritu, V. M. B. Sabado, and H. T. Sueno,  ``Classifying fake news in Filipino language using improved DistilBERT model,'' in Proc. 10th Int. Conf. Information Technology, Computer, and Electrical Engineering (ICITACEE), 2024, pp. 1--6.
\end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
