\documentclass[conference]{IEEEtran}

\makeatletter

\def\ps@IEEEtitlepagestyle{%
  \def\@oddfoot{\mycopyrightnotice}%
  \def\@evenfoot{}%
}
\def\mycopyrightnotice{%
  {\footnotesize XXX-X-XXXX-XXXX-X/XX/\$XX.00~\copyright~20XX IEEE\hfill}% <--- Change here
  \gdef\mycopyrightnotice{}
}


\usepackage{blindtext}
\usepackage{eso-pic}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{eso-pic}
\newcommand\AtPageUpperMyright[1]{\AtPageUpperLeft{%
 \put(\LenToUnit{0.17\paperwidth},\LenToUnit{-2cm}){%
     \parbox{0.9\textwidth}{\raggedleft\fontsize{8}{11}\selectfont #1}}%
 }}%
\newcommand{\conf}[1]{%
\AddToShipoutPictureBG*{%
\AtPageUpperMyright{#1}
}
}    
    
    
\begin{document}
\title{\vspace*{1cm} FaKe: Filipino Language Fake News Detection
System Using Machine Learning\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Rene Andre Jocsing}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences} \\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
rbjocsing@up.edu.ph}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Chancy Ponce de Leon}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences}\\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
cmponcedeleon@up.edu.ph}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Coebe Austin Lupac}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences} \\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
cvlupac1@up.edu.ph}
\and
\IEEEauthorblockN{4\textsuperscript{th} Francis Dimzon}
\IEEEauthorblockA{\textit{Division of Physical} \\
\textit{Sciences and Mathematics,} \\
\textit{College of Arts and Sciences}\\
\textit{University of the Philippines Visayas}\\
Iloilo, Philippines \\
fddimzon1@up.edu.ph}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}




\maketitle
\conf{\textit{  Proc. of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA 2026) \\ 
5-7 February 2026, Boracay-Philippines}}
\begin{abstract}
    Methods for curbing the spread of misinformation in the Philippines remain inadequate. The internet as a medium for fake news necessitates fast, automated, and accessible countermeasures. A precursor study from 2020 benchmarks Transfer Learning (TL) techniques in building Filipino language fake news classifiers from a low-resource dataset. Despite promising results, the models from the aforementioned study cannot be easily deployed for a wide audience. In this work, we show that robust fake news classifiers for a morphologically rich language can be constructed from lightweight machine learning models and a low-resource dataset. We show that these machine learning models can be successfully deployed on a stringent infrastructure. First, we construct a dataset of Filipino language news articles. We extract Filipino linguistic features from the dataset. Next, we train a Logistic Regression model, which achieves an accuracy of 95\% without hyperparameter tuning. Using this model, we build and deploy a system for classifying Filipino language fake news.
\end{abstract}

%\copyrightnotice{XXX-X-XXXX-XXXX-X/XX/\$XX.00 ©20XX IEEE}




\begin{IEEEkeywords}
natural language processing, machine learning, fake news in Filipino language, Filipino linguistic features
\end{IEEEkeywords}

\section{Introduction}
Misinformation ranks among the world's top global risks as fake news outlets see traffic. In the Philippines, journalists and political analysts speculate that Former President Rodrigo Duterte's landslide victory in the 2016 presidential elections has been brought about by paid trolls disseminating fake news through social media outlets \cite{b1}. Former President Duterte earned publicity by popularizing a depiction of the Philippines as a \textit{narco-state}.  Though he insists that this rhetoric is truth, the UN Office on Drugs and Crime reports otherwise—the Philippines' drug use prevalence is lower than the global average \cite{b2}.

A precursor work \cite{b3} explores Transfer Learning (TL) in building robust fake news classifiers for the morphologically rich Filipino language. Despite promising results, the deployment of the models (e.g., BERT) used in this work costs somewhere around 50,000 to 1,600,000 USD \cite{b4}. Hence, they cannot be easily deployed for consumer-level applications \cite{b3, b4}.

This work investigates lightweight machine learning models in tackling the classification of Filipino language fake news. A low-resource dataset of Filipino language news articles (Fake News Filipino 2024) is sourced from the internet to alleviate resource scarcity. A Logistic Regression model is trained on the combined Fake News Filipino and Fake News Filipino 2024 dataset, achieving an accuracy of 95\%. The authors successfully deploy the fake news classifier as a web extension (dubbed as FaKe) on a stringent infrastructure.

\section{Related Work}

Machine learning models have shown promise in automating fake news detection. A study \cite{b5} reports that Naive Bayes with \textit{n}-grams achieves 93\% accuracy on English datasets, while another study \cite{b6} finds that Support Vector Machine (SVM) reaches 96\% accuracy on a large corpora of about 20,000 articles. However, these studies focus on resource-rich English datasets.

For Filipino language news articles, a study \cite{b3} pioneers fake news detection using Byte-Pair Encoding (BPE) tokenization to handle morphologically rich language features and out of vocabulary (OOV) words. While another study \cite{b7} identifies 76 linguistic features for fake news detection in the Filipino context, their corpus uses English-language articles. Two studies \cite{b8, b9} develop Filipino-specific readability metrics and linguistic features including traditional features (word count, sentence count), syllabic patterns based on Philippine orthography, and morphological features from verb inflections. These studies identify polysyllable word count, sentence count, and average sentence length as top predictors for Filipino text readability. As of the time of writing, no standardized instrument or assessment tool exists for measuring the readability of texts in the Filipino language \cite{b8}. However, a study \cite{b10} proposes a formula for assessing the readability of modern Filipino texts, wherein readability score is a function of word count and words per sentence.

Despite these advances, Filipino linguistic features have not been applied in training Filipino language fake news classifiers. Moreover, deployment costs for complex models remain prohibitive (\$50,000 to \$1,600,000) \cite{b4}, necessitating the investigation of lightweight models for Filipino language fake news detection.

\section{Methodology}

\subsection{Data Collection}
Following a precursor study \cite{b3}, the authors constructed a new dataset of Filipino language news articles, Fake News Filipino 2024, containing 3,206 articles (1,603 fake, 1,603 authentic). Fake articles were sourced from fraudulent sites tagged by VERA Files while authentic articles were sourced from mainstream sources (Philippine Star, ABS-CBN). The news articles were UTF-8 encoded with minimal preprocessing to preserve features such as misspellings and punctuation.

\subsection{Feature Extraction and Model Training}
Text was tokenized using BPE, adhering to the methodology of the precursor study \cite{b3}. The authors extracted the following Filipino linguistic features \cite{b8,b9,b10}: traditional features (TRAD) (word, sentence, and character counts), syllabic features (SYLL) (Philippine orthography: \textit{v}, \textit{cv}, \textit{vc}, \textit{cvc}, etc.), and lexical features (LEX) (type-token ratio, parts-of-speech densities), morphological features (MORPH) (verb inflections). OOV words and stop words (SW) were also extracted. Macahilig's readability formula \cite{b10} was also used to extract readability scores from the corpus (READ). Tokens were vectorized with Term Frequency-Inverse Document Frequency (TF-IDF) (unigrams, bigrams, trigrams) and bag of words (BOW).

The authors trained four classifiers: Multinomial Naive Bayes, Logistic Regression, Random Forest, and SVC. Grid search with 5-fold cross-validation (CV) determined optimal hyperparameters. The classifiers were evaluated 30 times (5-fold CV x 6 runs) across three datasets: Fake News Filipino \cite{b3}, Fake News Filipino 2024, and their combined corpus. Two-way ANOVA with Bonferroni correction assessed performance differences and found these differences between models to be significant. The best model, Logistic Regression without hyperparameter tuning, achieved 95\% accuracy.

\subsection{System Deployment}
The best model was deployed as a web service using FastAPI on Render's free tier, interfaced through Tampermonkey. A web extension scrapes article text, sends it to the service for feature extraction and classification, then displays results. Due to infrastructure constraints, the deployed model excludes lexical/morphological features (requires deployment of Java runtime and 3+ minutes of processing time per article). The exclusion of these features during cross validation did not significantly affect the accuracy of the deployed model.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} J. Lanuza, J. Ong, and R. Tapsell, ``Evolutions of 'fake news' from the south: Tracking disinformation innovations and interventions between the 2016 and 2019 Philippines elections,'' Berkman Klein Center for Internet \& Society, Harvard University, 2019. [Online]. Available: https://cyber.harvard.edu/sites/default/files/2019-11/Comparative\%20Approaches\%20to\%20Disinformation\%20-\%20Jose\%20Mari\%20Hall\%20Lanuza\%20Slides.pdf
\bibitem{b2} A. Yee, ``Post-truth politics and fake news in Asia,'' Global Asia, vol. 12, pp. 66--71, 2017.
\bibitem{b3} J. C. B. Cruz, J. A. Tan, and C. Cheng, ``Localization of fake news detection via multitask transfer learning,'' in Proc. 12th Language Resources and Evaluation Conf., 2020, pp. 2596--2604.
\bibitem{b4} A. Paleyes, R.-G. Urma, and N. D. Lawrence, ``Challenges in deploying machine learning: A survey of case studies,'' ACM Comput. Surv., vol. 55, no. 6, pp. 1--29, 2022.
\bibitem{b5} J. Y. Khan, Md. T. Khondaker, S. Afroz, G. Uddin, and A. Iqbal, ``A benchmark study of machine learning models for online fake news detection,'' Mach. Learn. Appl., vol. 4, p. 100032, 2021.
\bibitem{b6} D. Choudhury and T. Acharjee, ``A novel approach to fake news detection in social networks using genetic algorithm applying machine learning classifiers,'' Multimedia Tools Appl., vol. 82, no. 6, pp. 9029--9045, 2022.
\bibitem{b7} A. C. T. Fernandez and M. Devaraj, ``Computing the linguistic-based cues of fake news in the Philippines towards its detection,'' in Proc. 9th Int. Conf. Web Intelligence, Mining and Semantics, 2019, pp. 1--9.
\bibitem{b8} J. M. Imperial and E. Ong, ``Exploring hybrid linguistic feature sets to measure Filipino text readability,'' in Proc. Int. Conf. Asian Language Processing (IALP), 2020, pp. 175--180.
\bibitem{b9} J. M. Imperial and E. Ong, ``Diverse linguistic features for assessing reading difficulty of educational Filipino texts,'' arXiv:2108.00241, 2021.
\bibitem{b10} H. B. Macahilig, ``A content-based readability formula for Filipino texts,'' The Normal Lights, vol. 8, 2015.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
