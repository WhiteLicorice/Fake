%   Filename    : chapter_2.tex 
\chapter{Review of Related Literature}
\label{sec:relatedlit}

In this review, we synthesize relevant information gathered from other related studies relating to machine learning models in the domain of natural language processing and fake news detection.

\section{Machine Learning in Automating Fake News Detection}

Machine learning models can be used in automating the detection of fake news. \citeA{khan-2021} benchmarks multiple machine learning methods for online fake news detection. They evaluate the models on three diverse datasets, using deep learning and machine learning models, including pre-trained models. Their results indicate that Naive Bayes with \textit{n}-gram performs the best out of all the traditional models, scoring a 93\% accuracy on the combined corpus. In Naive Bayes, the addition of sentiment and lexical features does not provide a significant increase in performance. However, with sentiment and lexical features, Support Vector Machine (SVM) and Logistic Regression models outperform other traditional machine learning models except for Naive Bayes at 72\% and 77\% accuracy respectively.

Naive Bayes models are simple probabilistic classifiers that apply Bayes' theorem, assuming strong independence between the features of the dataset (naive). \citeA{granik-2017} train a Naive Bayes classifier on articles sourced by Buzzfeed from Facebook. They achieve an accuracy of 75.40\% with their Naive Bayes classifier on the dataset. Their study is limited by the size of their dataset with only 2,000 articles. Furthermore, the articles in the dataset were not of significant length and were often merely previews of a lengthier news article. They also fail to take into account stop-words and rarely used words. Lastly, they do not perform stemming on their dataset. \citeA{krishna-2021} also delves into the application of Naive Bayes in fake news detection. They benchmark Multinomial Naive Bayes classifiers on a labeled dataset comprised of fake and authentic news articles. They perform stemming and pre-processing on the dataset before training. Their results show that Naive Bayes with Term Frequency-Inverter Document Frequency (TF-IDF) vectorizer scores an 85.7\% accuracy. With a count vectorizer, it scores an 89.3\% while with a hashing vectorizer, it scores 90.2\%. \citeA{krishna-2021} show that even a simple machine learning model such as Naive Bayes can perform well in detecting fake news. The features that they use in their study are N-count of the most used words, and-or phrases, casing, and stop-words such as \enquote{the} and \enquote{a}. However, their study fails to take other lexical features into account.

\citeA{tiwari-2020} train multiple machine learning classifiers on a dataset of real and fake news gathered from online sources, with a 50-50 split. They also utilize a count vectorizer, a TF-IDF vectorizer, and a hashing vectorizer similar to the study of \citeA{krishna-2021}. They compare numerous classifiers on the dataset. Logistic Regression performs the best with TF-IDF vectorizer, achieving a 71\% accuracy. \citeA{choudhury-2022} uses TF-IDF vectorizer to select features in their study. They compare SVM, Naive Bayes, Logistic Regression, and Random Forest models on three diverse datasets. They report that SVM attains the highest accuracy at 96\% on the aforementioned dataset, followed by a 95\% accuracy with Random Forest and Logistic Regression. Naive Bayes performs worse at only 90\% accuracy. However, the Fake News dataset is relatively large with around 20,000 English news articles. For low-resource languages like Filipino and with a small dataset, these classifiers may perform differently.

\section{Pre-processing and Feature Engineering}

\citeA{cruz2020localization} pre-process their dataset lightly to retain information such as capitalization, punctuation, and misspelled words. Pre-processing of the text is required to clean and analyze a dataset. The dataset they use in their study, Fake News Filipino, is primarily in Filipino, with the addition of borrowed English words. They use a Moses Tokenizer to derive descriptives from the dataset. In their data pre-processing, \citeA{cruz2020localization} only perform Byte-Pair Encoding (BPE) for tokenization. BPE is used as it allows their models to represent out-of-vocabulary (OOV) words. Additionally, BPE treats morphemes as the smallest token forms instead of whole words. Thus, it is useful for models in training classifiers for morphologically-rich languages.

\citeA{fernandez-2019} highlight 76 relevant linguistic features in identifying English language fake news in the Filipino context, sorted into 8 categories. These 8 categories are readability scores, linguistic dimensions, summative cues, affective cues, informality cues, cognitive cues, punctuation cues, and time-orientation cues. Features that do not require dictionaries such as readability scores, word count, syllables count, and words per sentence are implemented in Python using regular expressions and the Natural Language Toolkit. They extract dictionary-based features using The Linguistic Inquiry and Word Count software by \citeA{pennebaker-2007}. Of the seventy-six linguistic features, the top ten features with the highest mutual information scores are headline syllables count, headline word count, headline words per sentence, content present focus, headline difficult words count, headline function words, content difficult words count, and headline prepositions. \citeA{fernandez-2019} conclude that fake and authentic news differ significantly in terms of word count, sentence count, verb tenses, and readability scores. However, despite their corpus being in the Filipino context, the articles within are written in the English language. The relevant linguistic features for identifying Filipino language fake news may differ.

In a similar vein, a study by \citeA{fayaz-2022} lists a total of 23 features extracted from the ISOT Fake news data set. All features are not equally important. Some features contribute positively to model accuracy, while others negatively impact model performance. The authors reduce the feature space to 14 features using chi\textsuperscript{2}, univariate, feature importance, and information gain. Common features among the four selection techniques include word count, number of uppercase and lowercase characters, character count, sentence count, readability scores and indices, and TF-IDF.

However, the technology to extract these advanced linguistic features from Filipino text has been slow to develop, presenting a gap in the field of natural language processing. Specifically, popular metrics for readability scores are tailored for English texts and are not appropriate for Filipino texts. As of the time of writing, no standardized instrument or assessment tool exists for measuring the readability of texts in the Filipino language \cite{imperial-2020}. However, initial forays into this gap are promising. \citeA{Macahilig2015ACR} develop a formula for assessing the readability of modern Filipino texts, wherein readability score is a function of word count and words per sentence. \citeA{imperial-2020} and \citeA{imperial-2021} (two separate studies conducted by the same authors) present different linguistic feature sets in measuring Filipino text readability. They compare the different combinations of traditional features, lexical features, and language model features in measuring Filipino text readability. Using linear regression with the combination of all 25 predictors from the three feature sets produces the highest evaluation score of 72.0\%. Using Spearman correlation and information gain, they identify polysyllable word count, word count, sentence count, and average sentence length as the top predictors for the traditional features. For the language model features, the top predictors are all the three trigram models trained for the model. No predictor from the lexical features has been identified as a top predictor, which may imply that lexical features alone cannot be used for Filipino readability measurement. In addition to traditional, lexical, and language models, the authors append syllable patterns and morphological features to the feature space \cite{imperial-2021, imperial-2020}. As of the time of writing, no study implements these Filipino linguistic features in training machine learning models for the classification of Filipino language fake news articles.

\section{Summary}

The established literature bares a lack of investigation into machine learning models in the domain of Filipino language fake news. Linguistic features and readability metrics for Filipino texts are also in a stage of infancy. At present, a large dataset for Filipino language news articles does not exist. Despite these setbacks, \citeA{cruz2020localization} train deep learning models in classifying Filipino language fake news on a low-resource dataset. However, the models they train cannot be feasibly deployed in a fake news detection system due to unreasonable cost of deployment \cite{cruz2020localization, paleyes-2022}. Application of Filipino linguistic features highlighted in \citeA{imperial-2020} and \citeA{imperial-2021} has not been explored in training Filipino language fake news classifiers. In summary, while English language fake news classifiers and English linguistic features are well-trodden ground, the same cannot be said for their Filipino counterparts.